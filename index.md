## Детальный поурочный план

| Неделя | Занятие | Тема | Содержание |
| :--- | :--- | :--- | :--- |
| **1** | 1 | Введение. Исторический экскурс. | - |
| | 2 | Экосистема ИИ-разработчика. | - | 
| **2** | 3 | **Линейная алгебра в Python.** NumPy: Векторизация, Broadcast, Матричные операции. | Связь с Линалом (умножение матриц, транспонирование), оптимизация. |
| | 4 | Pandas: Структуры данных, индексация, агрегация. EDA. | Чистка данных, обработка пропусков, `groupby`. |
| **3** | 5 | **Статистика в коде.** SciPy: Распределения, статистические тесты. | Понятие p-value, t-тест, корреляция. Визуализация (Seaborn). |
| | 6 | **Градиентный спуск.** Производная, градиент. | Математический вывод, визуализация спуска. Learning Rate. |
| **4** | 7 | **Линейная регрессия.** Метод наименьших квадратов (OLS). Нормальное уравнение. | Математический вывод OLS, геометрический смысл. |
| | 8 | Реализация Линейной регрессии с нуля на NumPy. | Сравнение с `sklearn.LinearRegression`, разбор ДЗ по градиенту. |
| **5** | 9 | **Логистическая регрессия.** Сигмоида. Функция потерь (Кросс-энтропия). | Математический вывод кросс-энтропии, бинарная классификация. |
| | 10 | Регуляризация (L1, L2). Bias-Variance Tradeoff. | Математика регуляризации, подбор гиперпараметров. |
| **6** | 11 | **PCA (Метод главных компонент).** Собственные числа и векторы. | Математика PCA, сингулярное разложение. |
| | 12 | **Метрики классификации.** Матрица ошибок, ROC-AUC, Precision/Recall. | Анализ несбалансированных данных. |
| **7** | 13 | **Кластеризация (K-Means).** Евклидово расстояние. DBSCAN. | Выбор оптимального K, метрики кластеризации. |
| | 14 | **Промежуточный контроль.** | Обзор пройденного материала (NumPy, LogReg, Ансамбли). |
| **8** | 15 | **Введение в DL.** Нейрон, активация. **Backpropagation: Цепное правило.** | Математика Backprop, граф вычислений. |
| | 16 | **Фреймворки DL.** PyTorch/TensorFlow: Тензоры, Autograd, GPU. | Сравнение фреймворков, архитектура DL-кода. |
| **9** | 17 | **MLP и Цикл обучения.** Loss, Optimizer (SGD, Adam), Learning Rate Schedulers. | Детальный разбор цикла обучения в PyTorch. |
| | 18 | **CNN (Сверточные сети).** Свертка, Пулинг. Архитектуры (LeNet). | Понятие локального рецептивного поля, разделяемые веса. |
| **10** | 19 | **Продвинутые CNN.** ResNet, Inception. Transfer Learning (Fine-tuning). | Применение предобученных моделей, аугментация данных. |
| | 22 | **RNN/LSTM.** Проблема затухающего градиента. Ячейка памяти LSTM. | Математика гейтов LSTM. |
| **12** | 23 | **Трансформеры: Attention.** Механизм внимания (Query, Key, Value). | Scaled Dot-Product Attention, Self-Attention. |
| | 24 | **Архитектура Трансформера.** Encoder/Decoder. Позиционное кодирование. | Обзор BERT, GPT. |
| **13** | 25 | **Продвинутые темы DL.** GANs (Генератор/Дискриминатор). | Состязательное обучение, примеры применения. |
| | 26 | **Введение в Reinforcement Learning.** Агент, Среда, Награда. | Обзор алгоритмов (Q-Learning, DQN). |
| **14** | 27 | **Инженерия ML-систем.** MLOps: Docker, CI/CD, развертывание моделей. | Контейнеризация, FastAPI для API. |
| | 28 | **Этика и Безопасность ИИ.** Объяснимый ИИ (XAI). | Fairness, Interpretability. |
| **15** | 29 | **Проектный семинар 1.** Требования, структура отчета, критерии защиты. | Выбор темы, планирование, обзор литературы. |
| | 30 | **Проектный семинар 2.** Разбор сложных кейсов, Q&A. | Индивидуальная работа с группами/студентами. |
| **16** | 31 | **Проектный семинар 3.** Промежуточный отчет по проекту. | Проверка прогресса, исправление архитектурных ошибок. |
| | 32 | **Проектный семинар 4.** Финальная подготовка к защите. | Репетиция защиты, проверка презентаций. |
| **17** | 33 | Защита | **Защита итоговых проектов (Часть 1).** 
| | 34 | Защита | **Защита итоговых проектов (Часть 2).** 
| **18** | 35 | Защита | **Защита итоговых проектов (Часть 3).** 
| | 36 | Защита | **Защита итоговых проектов (Часть 4).**



## Cписок домашних заданий


| № ДЗ | Неделя выдачи | Тема | Тип задания | Фокус проверки на практике |
| :--- | :--- | :--- | :--- | :--- |
| **ДЗ 1** | 1 | **Инженерный старт** | Настройка окружения, Git-flow, реализация декоратора-таймера. | **Практика 1 (Нед. 2)**: Проверка Git-истории, чистоты кода (PEP8). |
| **ДЗ 2** | 2 | **Векторизация и EDA** | Реализация функции нормализации данных на чистом NumPy. Базовый EDA с Pandas и Seaborn. | **Практика 2 (Нед. 3)**: Сравнение скорости векторизованного и не-векторизованного кода. |
| **ДЗ 3** | 3 | **Градиентный спуск** | Реализация алгоритма градиентного спуска для функции $f(x, y) = x^2 + y^2$ с визуализацией траектории. | **Практика 3 (Нед. 4)**: Устное объяснение цепного правила и влияния Learning Rate. |
| **ДЗ 4** | 4 | **Линейная регрессия From Scratch** | Реализация OLS через нормальное уравнение на NumPy. | **Практика 4 (Нед. 5)**: Защита кода, объяснение математики OLS. |
| **ДЗ 5** | 5 | **Логистическая регрессия** | Реализация LogReg с нуля (с использованием градиентного спуска) и применение L2-регуляризации. | **Практика 5 (Нед. 6)**: Объяснение функции потерь (Кросс-энтропия) и эффекта регуляризации. |
| **ДЗ 6** | 6 | **Метрики и PCA** | Анализ несбалансированного датасета. Применение PCA для снижения размерности и визуализации. | **Практика 6 (Нед. 7)**: Защита LogReg, анализ ROC-AUC и выбор порога. |
| **ДЗ 7** | 7 | **Ансамблирование** | Применение Random Forest и XGBoost для решения задачи классификации. Тонкая настройка гиперпараметров с помощью `GridSearchCV`. | **Практика 7 (Нед. 8)**: Сравнение результатов ансамблей, объяснение принципов Bagging/Boosting. |
| **ДЗ 8** | 9 | **Backpropagation и PyTorch** | Реализация Backpropagation для одного нейрона на NumPy. Построение простой MLP на PyTorch. | **Практика 9 (Нед. 10)**: Защита кода Backprop, объяснение работы `Autograd` в PyTorch. |

## 4. Итоговый проект

*   **Защита**: Недели 17-18.
*   **Требования**: Использование DL-архитектуры и MLOps-инструментов (Docker, FastAPI).

